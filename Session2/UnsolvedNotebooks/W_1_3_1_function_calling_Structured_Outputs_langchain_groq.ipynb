{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mabdulhadi/360monodepth/blob/main/Session2/UnsolvedNotebooks/W_1_3_1_function_calling_Structured_Outputs_langchain_groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structured Outputs\n",
        "Guarantee model responses strictly conform to your JSON schema for reliable, type-safe data structures."
      ],
      "metadata": {
        "id": "ryO0E86QCicu"
      },
      "id": "ryO0E86QCicu"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d-tc35OALgk2",
        "outputId": "a27b0c33-4e50-4909-88fa-a35b85683792"
      },
      "id": "d-tc35OALgk2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_key = userdata.get('groqdummy')"
      ],
      "metadata": {
        "id": "9GhDg7SEL61r"
      },
      "id": "9GhDg7SEL61r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "from pydantic import BaseModel\n",
        "from typing import Literal\n",
        "import json\n",
        "\n",
        "\n",
        "client = Groq(api_key = groq_key)\n",
        "#Getting a structured response from unstructured text\n",
        "class ProductReview(BaseModel):\n",
        "    product_name: str\n",
        "    rating: float\n",
        "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"]\n",
        "    key_features: list[str]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"moonshotai/kimi-k2-instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Extract product review information from the text.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"I bought the UltraSound Headphones last week and I'm really impressed! The noise cancellation is amazing and the battery lasts all day. Sound quality is crisp and clear. I'd give it 4.5 out of 5 stars.\",\n",
        "        },\n",
        "    ],\n",
        "    response_format={\n",
        "        \"type\": \"json_schema\",\n",
        "        \"json_schema\": {\n",
        "            \"name\": \"product_review\",\n",
        "            \"schema\": ProductReview.model_json_schema()\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "review = ProductReview.model_validate(json.loads(response.choices[0].message.content))\n",
        "print(json.dumps(review.model_dump(), indent=2))"
      ],
      "metadata": {
        "id": "32fYgcu5Chqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc73483-7588-4170-9c15-924d73e754a0"
      },
      "id": "32fYgcu5Chqm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"product_name\": \"UltraSound Headphones\",\n",
            "  \"rating\": 4.5,\n",
            "  \"sentiment\": \"positive\",\n",
            "  \"key_features\": [\n",
            "    \"amazing noise cancellation\",\n",
            "    \"all-day battery life\",\n",
            "    \"crisp and clear sound quality\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Email Classification"
      ],
      "metadata": {
        "id": "r_JGzfk0C9VG"
      },
      "id": "r_JGzfk0C9VG"
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "from pydantic import BaseModel\n",
        "import json\n",
        "\n",
        "#client = Groq()\n",
        "\n",
        "class KeyEntity(BaseModel):\n",
        "    entity: str\n",
        "    type: str\n",
        "#You can classify emails into structured categories with confidence scores, priority levels, and suggested actions.\n",
        "class EmailClassification(BaseModel):\n",
        "    category: str\n",
        "    priority: str\n",
        "    confidence_score: float\n",
        "    sentiment: str\n",
        "    key_entities: list[KeyEntity]\n",
        "    suggested_actions: list[str]\n",
        "    requires_immediate_attention: bool\n",
        "    estimated_response_time: str\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"moonshotai/kimi-k2-instruct\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an email classification expert. Classify emails into structured categories with confidence scores, priority levels, and suggested actions.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": \"Subject: URGENT: Server downtime affecting production\\\\n\\\\nHi Team,\\\\n\\\\nOur main production server went down at 2:30 PM EST. Customer-facing services are currently unavailable. We need immediate action to restore services. Please join the emergency call.\\\\n\\\\nBest regards,\\\\nDevOps Team\"},\n",
        "    ],\n",
        "    response_format={\n",
        "        \"type\": \"json_schema\",\n",
        "        \"json_schema\": {\n",
        "            \"name\": \"email_classification\",\n",
        "            \"schema\": EmailClassification.model_json_schema()\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "email_classification = EmailClassification.model_validate(json.loads(response.choices[0].message.content))\n",
        "print(json.dumps(email_classification.model_dump(), indent=2))"
      ],
      "metadata": {
        "id": "JrAhAgHVC4Sn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b3e40f-88dd-4388-f44f-c7514ba9d662"
      },
      "id": "JrAhAgHVC4Sn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"category\": \"Incident Report\",\n",
            "  \"priority\": \"Critical\",\n",
            "  \"confidence_score\": 0.97,\n",
            "  \"sentiment\": \"Negative\",\n",
            "  \"key_entities\": [\n",
            "    {\n",
            "      \"entity\": \"production server\",\n",
            "      \"type\": \"Infrastructure\"\n",
            "    },\n",
            "    {\n",
            "      \"entity\": \"2:30 PM EST\",\n",
            "      \"type\": \"Time\"\n",
            "    },\n",
            "    {\n",
            "      \"entity\": \"customer-facing services\",\n",
            "      \"type\": \"Service\"\n",
            "    },\n",
            "    {\n",
            "      \"entity\": \"DevOps Team\",\n",
            "      \"type\": \"Team\"\n",
            "    }\n",
            "  ],\n",
            "  \"suggested_actions\": [\n",
            "    \"Join the emergency call immediately\",\n",
            "    \"Assess server status and initiate recovery procedures\",\n",
            "    \"Communicate ETA updates to stakeholders\",\n",
            "    \"Post-incident review and root-cause analysis\"\n",
            "  ],\n",
            "  \"requires_immediate_attention\": true,\n",
            "  \"estimated_response_time\": \"<15 minutes\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Ticket Classification"
      ],
      "metadata": {
        "id": "73gr-hr0Do7e"
      },
      "id": "73gr-hr0Do7e"
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "from enum import Enum\n",
        "import json\n",
        "\n",
        "#client = Groq()\n",
        "\n",
        "class SupportCategory(str, Enum):\n",
        "    API = \"api\"\n",
        "    BILLING = \"billing\"\n",
        "    ACCOUNT = \"account\"\n",
        "    BUG = \"bug\"\n",
        "    FEATURE_REQUEST = \"feature_request\"\n",
        "    INTEGRATION = \"integration\"\n",
        "    SECURITY = \"security\"\n",
        "    PERFORMANCE = \"performance\"\n",
        "\n",
        "class Priority(str, Enum):\n",
        "    LOW = \"low\"\n",
        "    MEDIUM = \"medium\"\n",
        "    HIGH = \"high\"\n",
        "    CRITICAL = \"critical\"\n",
        "\n",
        "class CustomerTier(str, Enum):\n",
        "    FREE = \"free\"\n",
        "    PAID = \"paid\"\n",
        "    ENTERPRISE = \"enterprise\"\n",
        "    TRIAL = \"trial\"\n",
        "\n",
        "class CustomerInfo(BaseModel):\n",
        "    name: str\n",
        "    company: Optional[str] = None\n",
        "    tier: CustomerTier\n",
        "\n",
        "class TechnicalDetail(BaseModel):\n",
        "    component: str\n",
        "    error_code: Optional[str] = None\n",
        "    description: str\n",
        "\n",
        "class SupportTicket(BaseModel):\n",
        "    category: SupportCategory\n",
        "    priority: Priority\n",
        "    urgency_score: float\n",
        "    customer_info: CustomerInfo\n",
        "    technical_details: List[TechnicalDetail]\n",
        "    keywords: List[str]\n",
        "    requires_escalation: bool\n",
        "    estimated_resolution_hours: float\n",
        "    follow_up_date: Optional[str] = Field(None, description=\"ISO datetime string\")\n",
        "    summary: str\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"moonshotai/kimi-k2-instruct\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"You are a customer support ticket classifier for SaaS companies.\n",
        "                         Analyze support tickets and categorize them for efficient routing and resolution.\n",
        "                         Output JSON only using the schema provided.\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"Hello! I love your product and have been using it for 6 months.\n",
        "                         I was wondering if you could add a dark mode feature to the dashboard?\n",
        "                         Many of our team members work late hours and would really appreciate this.\n",
        "                         Also, it would be great to have keyboard shortcuts for common actions.\n",
        "                         Not urgent, but would be a nice enhancement!\n",
        "                         Best, Mike from StartupXYZ\"\"\"\n",
        "        },\n",
        "    ],\n",
        "    response_format={\n",
        "        \"type\": \"json_schema\",\n",
        "        \"json_schema\": {\n",
        "            \"name\": \"support_ticket_classification\",\n",
        "            \"schema\": SupportTicket.model_json_schema()\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_result = json.loads(response.choices[0].message.content or \"{}\")\n",
        "result = SupportTicket.model_validate(raw_result)\n",
        "print(result.model_dump_json(indent=2))"
      ],
      "metadata": {
        "id": "AbXht3y5DhkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e0d056-88d2-4918-d57b-a4c88d6a3657"
      },
      "id": "AbXht3y5DhkO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"category\": \"feature_request\",\n",
            "  \"priority\": \"low\",\n",
            "  \"urgency_score\": 2.0,\n",
            "  \"customer_info\": {\n",
            "    \"name\": \"Mike\",\n",
            "    \"company\": \"StartupXYZ\",\n",
            "    \"tier\": \"paid\"\n",
            "  },\n",
            "  \"technical_details\": [\n",
            "    {\n",
            "      \"component\": \"dashboard\",\n",
            "      \"error_code\": null,\n",
            "      \"description\": \"Request to add dark mode theme support for the dashboard UI\"\n",
            "    },\n",
            "    {\n",
            "      \"component\": \"keyboard_shortcuts\",\n",
            "      \"error_code\": null,\n",
            "      \"description\": \"Request to add keyboard shortcuts for common actions\"\n",
            "    }\n",
            "  ],\n",
            "  \"keywords\": [\n",
            "    \"dark_mode\",\n",
            "    \"dashboard\",\n",
            "    \"keyboard_shortcuts\",\n",
            "    \"enhancement\",\n",
            "    \"UI\"\n",
            "  ],\n",
            "  \"requires_escalation\": false,\n",
            "  \"estimated_resolution_hours\": 120.0,\n",
            "  \"follow_up_date\": null,\n",
            "  \"summary\": \"Customer requesting dark mode for dashboard and keyboard shortcuts for common actions. Non-urgent enhancement request from 6-month user.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def koi_fun(input_variables,two, two,):\n",
        "\n",
        "\n",
        "  #body functionality  API, calcualtion, data loader\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "collapsed": true,
        "id": "1gLPaWYsN7Gb",
        "outputId": "d9443dfc-eb50-486d-fd22-758d586a1f61"
      },
      "id": "1gLPaWYsN7Gb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-1653422645.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1653422645.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def koi_fun(input_variables,one, two,):\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d59b14-c434-4359-be8e-4a21378e762f",
      "metadata": {
        "id": "50d59b14-c434-4359-be8e-4a21378e762f"
      },
      "source": [
        "# How to do tool/function calling\n",
        "\n",
        "\n",
        "We use the term tool calling interchangeably with function calling. Although\n",
        "function calling is sometimes meant to refer to invocations of a single function,\n",
        "we treat all models as though they can return multiple tool or function calls in\n",
        "each message.\n",
        ":::\n",
        "\n",
        "Tool calling allows a model to respond to a given prompt by generating output that\n",
        "matches a user-defined schema. While the name implies that the model is performing\n",
        "some action, this is actually not the case! The model is coming up with the\n",
        "arguments to a tool, and actually running the tool (or not) is up to the user -\n",
        "for example, if you want to [extract output matching some schema](/docs/tutorials/extraction)\n",
        "from unstructured text, you could give the model an \"extraction\" tool that takes\n",
        "parameters matching the desired schema, then treat the generated output as your final\n",
        "result.\n",
        "\n",
        "A tool call includes a name, arguments dict, and an optional identifier. The\n",
        "arguments dict is structured `{argument_name: argument_value}`.\n",
        "\n",
        "Many LLM providers, including [Anthropic](https://www.anthropic.com/),\n",
        "[Cohere](https://cohere.com/), [Google](https://cloud.google.com/vertex-ai),\n",
        "[Mistral](https://mistral.ai/), [OpenAI](https://openai.com/), and others,\n",
        "support variants of a tool calling feature. These features typically allow requests\n",
        "to the LLM to include available tools and their schemas, and for responses to include\n",
        "calls to these tools. For instance, given a search engine tool, an LLM might handle a\n",
        "query by first issuing a call to the search engine. The system calling the LLM can\n",
        "receive the tool call, execute it, and return the output to the LLM to inform its\n",
        "response. LangChain includes a suite of [built-in tools](/docs/integrations/tools/)\n",
        "and supports several methods for defining your own [custom tools](/docs/how_to/custom_tools).\n",
        "Tool-calling is extremely useful for building [tool-using chains and agents](/docs/how_to#tools),\n",
        "and for getting structured outputs from models more generally.\n",
        "\n",
        "Providers adopt different conventions for formatting tool schemas and tool calls.\n",
        "For instance, Anthropic returns tool calls as parsed structures within a larger content block:\n",
        "```python\n",
        "[\n",
        "  {\n",
        "    \"text\": \"<thinking>\\nI should use a tool.\\n</thinking>\",\n",
        "    \"type\": \"text\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"id_value\",\n",
        "    \"input\": {\"arg_name\": \"arg_value\"},\n",
        "    \"name\": \"tool_name\",\n",
        "    \"type\": \"tool_use\"\n",
        "  }\n",
        "]\n",
        "```\n",
        "whereas OpenAI separates tool calls into a distinct parameter, with arguments as JSON strings:\n",
        "```python\n",
        "{\n",
        "  \"tool_calls\": [\n",
        "    {\n",
        "      \"id\": \"id_value\",\n",
        "      \"function\": {\n",
        "        \"arguments\": '{\"arg_name\": \"arg_value\"}',\n",
        "        \"name\": \"tool_name\"\n",
        "      },\n",
        "      \"type\": \"function\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "LangChain implements standard interfaces for defining tools, passing them to LLMs,\n",
        "and representing tool calls.\n",
        "\n",
        "## Passing tools to LLMs\n",
        "\n",
        "Chat models supporting tool calling features implement a `.bind_tools` method, which\n",
        "receives a list of LangChain [tool objects](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.BaseTool.html#langchain_core.tools.BaseTool)\n",
        "and binds them to the chat model in its expected format. Subsequent invocations of the\n",
        "chat model will include tool schemas in its calls to the LLM.\n",
        "\n",
        "For example, we can define the schema for custom tools using the `@tool` decorator\n",
        "on Python functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "841dca72-1b57-4a42-8e22-da4835c4cfe0",
      "metadata": {
        "id": "841dca72-1b57-4a42-8e22-da4835c4cfe0"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplies a and b.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "tools = [add, multiply]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48058b7d-048d-48e6-a272-3931ad7ad146",
      "metadata": {
        "id": "48058b7d-048d-48e6-a272-3931ad7ad146"
      },
      "source": [
        "Or below, we define the schema using Pydantic:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca56328-85e4-4839-97b7-b5dc55920602",
      "metadata": {
        "id": "fca56328-85e4-4839-97b7-b5dc55920602"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Note that the docstrings here are crucial, as they will be passed along\n",
        "# to the model along with the class name.\n",
        "class Add(BaseModel):\n",
        "    \"\"\"Add two integers together.\"\"\"\n",
        "\n",
        "    a: int = Field(..., description=\"First integer\")\n",
        "    b: int = Field(..., description=\"Second integer\")\n",
        "\n",
        "\n",
        "class Multiply(BaseModel):\n",
        "    \"\"\"Multiply two integers together.\"\"\"\n",
        "\n",
        "    a: int = Field(..., description=\"First integer\")\n",
        "    b: int = Field(..., description=\"Second integer\")\n",
        "\n",
        "\n",
        "tools = [Add, Multiply]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead9068d-11f6-42f3-a508-3c1830189947",
      "metadata": {
        "id": "ead9068d-11f6-42f3-a508-3c1830189947"
      },
      "source": [
        "We can bind them to chat models as follows:\n",
        "\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs\n",
        "  customVarName=\"llm\"\n",
        "  overrideParams={{fireworks: {model: \"accounts/fireworks/models/firefunction-v1\", kwargs: \"temperature=0\"}}}\n",
        "/>\n",
        "\n",
        "We can use the `bind_tools()` method to handle converting\n",
        "`Multiply` to a \"tool\" and binding it to the model (i.e.,\n",
        "passing it in each time the model is invoked)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44eb8327-a03d-4c7c-945e-30f13f455346",
      "metadata": {
        "id": "44eb8327-a03d-4c7c-945e-30f13f455346"
      },
      "outputs": [],
      "source": [
        "# # | echo: false\n",
        "# # | output: false\n",
        "\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"langchain[groq]\""
      ],
      "metadata": {
        "id": "NlcmGfE8n2tK"
      },
      "id": "NlcmGfE8n2tK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import userdata\n",
        "# key = userdata.get('groqdummy')"
      ],
      "metadata": {
        "id": "65MKo0oln6ER"
      },
      "id": "65MKo0oln6ER",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "  os.environ[\"GROQ_API_KEY\"] = groq_key\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"moonshotai/kimi-k2-instruct\", model_provider=\"groq\")"
      ],
      "metadata": {
        "id": "ioLnbvsKoE-x"
      },
      "id": "ioLnbvsKoE-x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af2a83ac-e43f-43ce-b107-9ed8376bfb75",
      "metadata": {
        "id": "af2a83ac-e43f-43ce-b107-9ed8376bfb75"
      },
      "outputs": [],
      "source": [
        "llm_with_tools = llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16208230-f64f-4935-9aa1-280a91f34ba3",
      "metadata": {
        "id": "16208230-f64f-4935-9aa1-280a91f34ba3"
      },
      "source": [
        "## Tool calls\n",
        "\n",
        "If tool calls are included in a LLM response, they are attached to the corresponding\n",
        "[message](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html#langchain_core.messages.ai.AIMessage)\n",
        "or [message chunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html#langchain_core.messages.ai.AIMessageChunk)\n",
        "as a list of [tool call](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolCall.html#langchain_core.messages.tool.ToolCall)\n",
        "objects in the `.tool_calls` attribute. A `ToolCall` is a typed dict that includes a\n",
        "tool name, dict of argument values, and (optionally) an identifier. Messages with no\n",
        "tool calls default to an empty list for this attribute.\n",
        "\n",
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1640a4b4-c201-4b23-b257-738d854fb9fd",
      "metadata": {
        "id": "1640a4b4-c201-4b23-b257-738d854fb9fd",
        "outputId": "eefd14ca-cfa7-446d-f2fc-ba0f87082188",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'Multiply',\n",
              "  'args': {'a': 2, 'b': 2},\n",
              "  'id': 'functions.Multiply:0',\n",
              "  'type': 'tool_call'},\n",
              " {'name': 'Add',\n",
              "  'args': {'a': 3, 'b': 5},\n",
              "  'id': 'functions.Add:1',\n",
              "  'type': 'tool_call'},\n",
              " {'name': 'Add',\n",
              "  'args': {'a': 5, 'b': 6},\n",
              "  'id': 'functions.Add:2',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "query = \"what is 2*2 and 3 + 5 and 5 + 6 ?\"\n",
        "\n",
        "llm_with_tools.invoke(query).tool_calls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac3ff0fe-5119-46b8-a578-530245bff23f",
      "metadata": {
        "id": "ac3ff0fe-5119-46b8-a578-530245bff23f"
      },
      "source": [
        "The `.tool_calls` attribute should contain valid tool calls. Note that on occasion,\n",
        "model providers may output malformed tool calls (e.g., arguments that are not\n",
        "valid JSON). When parsing fails in these cases, instances\n",
        "of [InvalidToolCall](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.InvalidToolCall.html#langchain_core.messages.tool.InvalidToolCall)\n",
        "are populated in the `.invalid_tool_calls` attribute. An `InvalidToolCall` can have\n",
        "a name, string arguments, identifier, and error message.\n",
        "\n",
        "If desired, [output parsers](/docs/how_to#output-parsers) can further\n",
        "process the output. For example, we can convert back to the original Pydantic class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca15fcad-74fe-4109-a1b1-346c3eefe238",
      "metadata": {
        "id": "ca15fcad-74fe-4109-a1b1-346c3eefe238",
        "outputId": "b2026dbf-4f45-4a51-d3e9-74ad44da0eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'multiply'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-aaec54a14499>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_with_tools\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mPydanticToolsParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMultiply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3033\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3034\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3035\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3036\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m     ) -> T:\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 lambda inner_input: self.parse_result(\n\u001b[1;32m    198\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mChatGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1928\u001b[0m                 output = cast(\n\u001b[1;32m   1929\u001b[0m                     \u001b[0;34m\"Output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1930\u001b[0;31m                     context.run(\n\u001b[0m\u001b[1;32m   1931\u001b[0m                         \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m                         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             return self._call_with_config(\n\u001b[0;32m--> 197\u001b[0;31m                 lambda inner_input: self.parse_result(\n\u001b[0m\u001b[1;32m    198\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mChatGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 ),\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/output_parsers/openai_tools.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mpydantic_objects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValidationError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'multiply'"
          ]
        }
      ],
      "source": [
        "# from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
        "\n",
        "# chain = llm_with_tools | PydanticToolsParser(tools=[Multiply, Add])\n",
        "# chain.invoke(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97a0c977-0c3c-4011-b49b-db98c609d0ce",
      "metadata": {
        "id": "97a0c977-0c3c-4011-b49b-db98c609d0ce"
      },
      "source": [
        "## Passing tool outputs to model\n",
        "\n",
        "If we're using the model-generated tool invocations to actually call tools and want to pass the tool results back to the model, we can do so using `ToolMessage`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48049192-be28-42ab-9a44-d897924e67cd",
      "metadata": {
        "id": "48049192-be28-42ab-9a44-d897924e67cd",
        "outputId": "82e5287d-408b-4cbf-e974-67f645850915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"I'll multiply 4 and 5 for you.\" additional_kwargs={'tool_calls': [{'id': 'functions.Multiply:0', 'function': {'arguments': '{\"a\":4,\"b\":5}', 'name': 'Multiply'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 197, 'total_tokens': 230, 'completion_time': 0.074226871, 'prompt_time': 2.6051005160000003, 'queue_time': 0.115586136, 'total_time': 2.679327387}, 'model_name': 'moonshotai/kimi-k2-instruct', 'system_fingerprint': 'fp_b8565bb333', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--70950d4b-a450-46aa-a0ca-325e7af9210f-0' tool_calls=[{'name': 'Multiply', 'args': {'a': 4, 'b': 5}, 'id': 'functions.Multiply:0', 'type': 'tool_call'}] usage_metadata={'input_tokens': 197, 'output_tokens': 33, 'total_tokens': 230}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMessage(content='20', tool_call_id='functions.Multiply:0')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, ToolMessage\n",
        "query = \"what is 4 multiply 5\"\n",
        "messages = [HumanMessage(query)]\n",
        "ai_msg = llm_with_tools.invoke(messages)\n",
        "print(ai_msg)\n",
        "messages.append(ai_msg)\n",
        "for tool_call in ai_msg.tool_calls:\n",
        "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
        "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
        "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
        "messages[-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages[-1].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BluPEbDerX6p",
        "outputId": "f8c99dc8-c2aa-4c59-d3fc-4c820fcfe82a"
      },
      "id": "BluPEbDerX6p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2000'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611e0f36-d736-48d1-bca1-1cec51d223f3",
      "metadata": {
        "id": "611e0f36-d736-48d1-bca1-1cec51d223f3",
        "outputId": "083a01d0-12c0-4272-f846-b17f11e34577",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_k4t2', 'function': {'arguments': '{\"a\":2000,\"b\":0}', 'name': 'Add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 325, 'total_tokens': 345, 'completion_time': 0.037910674, 'prompt_time': 0.017878976, 'queue_time': 0.209801022, 'total_time': 0.05578965}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_a4265e44d5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--2d7a0e98-503e-4987-81c0-edf85b43ab8a-0', tool_calls=[{'name': 'Add', 'args': {'a': 2000, 'b': 0}, 'id': 'call_k4t2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 325, 'output_tokens': 20, 'total_tokens': 345})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#llm_with_tools.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sr-6CDK_mwjU"
      },
      "id": "sr-6CDK_mwjU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}